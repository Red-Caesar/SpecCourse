models: [
    "meta-llama/Llama-3.2-1B-Instruct",
    "models/Llama-3.2-1B-Instruct-scheme-FP8",
    "models/Llama-3.2-1B-Instruct-scheme-INT8",
    "models/Llama-3.2-1B-Instruct-scheme-INT4",
    "models/Llama-3.2-1B-Instruct-scheme-sparse_05",
    "models/Llama-3.2-1B-Instruct-scheme-sparse_05_FP8",
    "models/Llama-3.2-1B-Instruct-scheme-sparse_05_INT8",
    "models/Llama-3.2-1B-Instruct-scheme-sparse_05_INT4",
    "meta-llama/Llama-3.1-8B-Instruct",
    "models/Llama-3.1-8B-Instruct-scheme-FP8",
    "models/Llama-3.1-8B-Instruct-scheme-INT8",
    "models/Llama-3.1-8B-Instruct-scheme-INT4",
    "models/Llama-3.1-8B-Instruct-scheme-sparse_05",
    # "models/Llama-3.1-8B-Instruct-scheme-sparse_05_FP8",
    # "models/Llama-3.1-8B-Instruct-scheme-sparse_05_INT8",
    # "models/Llama-3.1-8B-Instruct-scheme-sparse_05_INT4",
  ]

lm_eval_args:
  tasks: "gsm8k"
  batch_size: "128"
  apply_chat_template: ""
  fewshot_as_multiturn: ""
  log_samples: ""
  output_path: "./results/gsm8k"
  num_fewshot: "0"
